{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# llm = Ollama(model=\"llama2\", request_timeout=600) #base_url = 'http://localhost:11434',\n",
    "\n",
    "# response = llm.complete(\"Do you know about Claude-3?\") ## stream_complete() for streaming response\n",
    "\n",
    "# response\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Documents/ChatBot/Llama2/v_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "from pypdf import PdfReader\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Creating a pdf reader object\n",
    "reader = PdfReader('faq.pdf')\n",
    "\n",
    "# Extracting text from all pages\n",
    "all_text = \"\"\n",
    "for page in reader.pages:\n",
    "    all_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "# Setting up the embedding model and LLM\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "Settings.llm = Ollama(model=\"llama2\", request_timeout=600)\n",
    "\n",
    "# Creating a Document object\n",
    "doc = Document(text=all_text)\n",
    "\n",
    "# Creating the VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pradhan Mantri Awas Yojana-Urban 2.0 (PMAY-U 2.0) is a central government scheme aimed at providing affordable housing to eligible urban families in India. The scheme was launched in 2024 and will be implemented for a period of 5 years until 2029. PMAY-U 2.0 provides financial assistance to states/UTs to construct, purchase or rent affordable houses in urban areas. The scheme focuses on providing housing opportunities to the Economically Weaker Section (EWS), Low Income Group (LIG), and Middle Income Group (MIG) segments.\n"
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# Function to get response with LLM validation\n",
    "def get_validated_response(question):\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_nodes = retriever.retrieve(question)\n",
    "    \n",
    "    # Prepare context for LLM\n",
    "    context = \"\\n\".join([node.text for node in retrieved_nodes])\n",
    "    \n",
    "    # Prepare prompt for LLM\n",
    "    prompt = f\"\"\"Given the following context and question, provide a accurate and concise answer. If the context doesn't contain enough information to answer the question confidently, state that clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Use LLM to generate a validated response\n",
    "    llm = Ollama(model=\"llama2\", request_timeout=600, temperature=0.1)\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "# question = \"i am a farmar, how can i apply for this scheme??\"\n",
    "# response = get_validated_response(question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"i am a farmar, how can i apply for this scheme??\"\n",
    "response = get_validated_response(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
